---
title: "AI 부트캠프 3주 : 선형대수"
excerpt: "코드스테이츠와 함께하는 'AI 부트캠프' 3주차 회고"
categories:
  - Coding
  - study
tags:
  - 개발일지
  - 코딩
  - AI 부트캠프
  - 코드스테이츠

header:
  teaser: https://www.codestates.com/assets/codestates-ci.png

last_modified_at: 2022-12-29
---

<div style="display:block; margin:auto; width:500px">
  <img alt="image" src = "https://lh3.googleusercontent.com/fife/AAbDypDjrvVKMbWfsnNNKNodjhtLjxdzzmbl4orvRm0R7U_JjhqakG2_HwFvsZVop3HRkUL6m_MO_uha5GmcPiKWDuZqv5qwW6VcgpSmW7nuh1VLy3jp0Wwr5iFPsdD3Gv4_eYjDnaZpnSRsDyRrVKXjIVISY1PtL4CBu80Q7bTvsFIxOnf69jkb_EvOy5HsgP3HIemdkyN2PXIdFhg9mcKmdS44ajQebCCqK6eMid3ZL6rJX9dc-2_Uzf4bXJQzGPIRROoiVAyB8ddm1a0m6u4db36diW2UhV9doteK01Wad8SByO7zQ-jj9UdsVwX4_w_wBSWHI1UaL3C3Jlt3hmrl1Y4w7Pz0toiNxL7Pu2wZPAYMQk3P4piPETXVGKld5SDwDKa6_R23Qmu_llqKFMXeMuq2hUz7dFJ6yQ4amWtT5WLflmVC6zm0vKicbAJ_74vTROuJuo2IIKQoDS15TAXHTdYF2wK5gJlfAonGrtkYtCqIkz3Tf_mktx8YwqdTLTU5v2fMeRyfEuBCCSueRKN7axug0tCRsPpXfN5X_N_0M_dXGOCjOwZYaNwa19P2yVJnU6ZoFVnxKSfH23QDaDMuZv81zILcQ2LHVAYXZtRt3aRl7OfcaIUNmF0Q-eOH3cAQewkAxXfwzMuP17TYuyOaDcO_8nZc6dgxS4GT4-QnrgxHU719xkbJ7LMw-0s_bj7T_ANf-uAbA_Y4CYumO5sqAKr6hp2kz9I7yVpxA4gsWxnxaRZ4nymq3cnnVejQaR6i1AVfnGc1FanMJiJD4BMiVOcZuyFNBl31dHX07v_hE2Wy0tLmSTE1PPRW5QYRc88igsBoPbvrBQ9LHdF27zQqPa2a0hlwGpseMjKLPcDdcQrROlvLpNOsrz2aQDEfdnTN-tfC2PMty1Q8_rcz4mqph1lHvVtgCAAavek8-u6pLHDFy-uRlYJbMrn8u4Nv5UsHV2Y33qF7S3LSFJaLFjZI6-nQjCW_fnurkV7ZAXXrG6hsYjOZKS3ObAuJZTflCfUavR9OthXp5VF-Mt8nk4Pzx3Z25QYsdnwZTS9PGzbBHYpSA5NihHWTDxLDKMLEw859XzHUE2T_M_jHhqyinhH1TwY9VhVKZzsjpAroIPNvzs1dsxRGH-WWM7g5YXKzu9vB2uNfkUeM297S1MjvqDcSHcN-49seUitRQDEYG-nlte2JLjyUua5ym-G6CS_quss8cwoxJHyLrMblUw4oOSBjAKb_NpaHLE3LO2s0bKIjBgXzrqCO1Dr5wsublbMJQ4CiSo1MU4PqXc7LoF8GYh5-iblW45MfHzPC_U-liq0jTJu2tiumKVzDQZw9LM7JP5v7GdEc2HmwH1i8s38Hqj_Df6ZPzyf2Vf803oK1Z7Oj6zI19rSnm0CBro9vLlo3VJEoxQZEiumuK4rC6IT2FXPaiS3E-3EaWaalhOv_SmRsZWkoNPXj10lEI-D4RRIVHkcjk7_HKr9ch0d6Skjg8cVW6qHuLo57tw0NxE0Ie5UpFOLG-gAcImvX4K5Seplg=w1263-h780">  
</div>


# [코드스테이츠](https://www.codestates.com/)와 함께하는 'AI 부프캠프' 3주차

## 전반적 회고
> 코드스테이츠 AI부트캠프 AIsection1 sprint3 과정에 참여하여 머신러닝을 위한 기초적인 공부(선형대수, 차원축소, 경사하강법)와 실습(PCA, 클러스터링)을 진행하였다.  
고등학교수학과정이 나와서 당황했다. 손으로 계산하는 것은 어느정도 할 수 있겠는데, 코드로 구현하려니 많이 어색했다. 하지만 꾸준하게 라이브러리를 활용하여 계산하니 많이 익숙해져서 편한느낌, <mark>내 것이 된 느낌</mark>이 들었다.  
고등학교 수학과정에서 배웠던 내용들이 많이 나왔다. 그리고 직접 수학공식을 활용하니 점점 익숙해지고, 수학공식이 내것이 된 느낌이었다. 아는 것에서 만족하지 않고 익숙해져서 내것으로 만들어야겠다.  
다양한 라이브러리들이 편리한 기능을 제공한다. 그 편리한 기능에 익숙해져야겠다. 어떤 라이브러리들이 있는지 살피고, 꾸준하게 실습하면서 익숙해지면, 비로소 그 기능들이 내것이 될 것이다.

## 학습내용
### 학습 기간
- Sprint 2기간 : 2022.12.23.(금) ~ 29.(목)

### Linear Algebra
- python의 list와 numpy의 array의 차이
  - list : 수치적 연산이 불가능
  - array : 수치적 연산이 가능

- Scalar : 숫자
- Vector : 순서를 갖는 1차원 형태의 배열
  - 벡터의 크기 : 벡터의 길이, Norm, length, Magnitude
  - 벡터의 내적 : Dot Product, 곱하고더하고
  - 벡터의 직교 : 내적이  0
  - 단위 벡터 : Unit Vector, 길이가 1인 벡터
- Matrix : 행과 열
  - 행렬의 전치 : Transpose
    - $A^T$
    - $(A^T)^T=A$ 
    - [`np.transpose()`](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html)
  - 행렬곱(Matrix Multiplication)
    - [`np.matmul()`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html)
  - 정사각 행렬(Square Matrix)
    - 행과 열이ㅡ 수가 동일한 메트릭스
  - 대각 행렬(Diagonal Matrix)
    - 주 대각선(principal diagonal)을 제외한 모든 성분이  0 인 정사각 행렬
    > $D =
\begin{bmatrix}
a_{1,1} & 0 & 0 \\
0 & a_{2,2} & 0 \\
0 & 0 & a_{3,3} 
\end{bmatrix}$
  - 단위 행렬(Identity Matrix)
    - 대각 행렬 중에서 주 대각선 성분이 모두  1 인 매트릭스
    - [`np.identity()`](https://numpy.org/doc/stable/reference/generated/numpy.identity.html) 또는 [`np.eye()`](https://numpy.org/devdocs/reference/generated/numpy.eye.html)
  - 역행렬(Inverse)
    - 임의의 정사각 행렬에 대하여 곱했을 때 단위 행렬이 되도록 하는 행렬
    - $A^{-1}$
    - [`np.linalg.inv()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html)
  - 행렬식(Determinant)
    - 정사각 행렬 $A$에 대해서 det($A$) 또는 $|A|$로 표기
    - [`np.linalg.det()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.det.html)
    - 행렬식이  0 이면 역행렬이 존재하지 않음
- Span
  - 주어진 두 벡터의 조합으로 만들 수 있는 모든 가능한 벡터의 집합
  - 두 벡터가 선형 독립이면 2차원 span
  - 세 벡터가 선형 독립이면 3차원 span
  - 선형독립인 두 벡터(빨강, 초록)으로 만들 수 있는 벡터(파랑)들의 공간(연두)  
<div style="display:block; margin:auto; width:500px">
  <img alt="image" src = "https://lh3.googleusercontent.com/fife/AAbDypC8NdTETIwxAh0ykeCzN3pzUuDtM4x5ej8c7lPAmq8NfHVXQnHhJbfFkeioiBW_MDm5b9s-gqOWtq9yYfE9i8D_ocNHPUa-IjoBavk9_Pj-Q-MfMF8Du5cxsgNpaZ09Su7iCsCFmO6ygyj_YSkgMsfGOrer5yv7tUdjf-mBDqP5uS_I2mIze9I_fprLcpJNnhsY8jHiKc4Hcep08LYq9UZb8w58R21CLwFPTv2AAtrgbeeWe5_1KrL0h0MhK4fFsr3Bs9cLwmI54FNpfoJ0K8VP-zGbOIpFZczn4Ev8JVLjgLBzQ8MgrKe0SkALI1Fa9v_rmLnKLGEBTb5D6V_9JXGxXHdeatg3SyP6nMRJ_2efUFFRCR78PzcWJPWLhGlT9wIJefm5XUnHmlkXhV84pnvZOoB9lBhQYE4_tet2mglPuLAIleU9v8bwDsY0r-ZJcgpVRhyPoPW1xCee8BRp5hLPXQp42z_3La3whQiV0leLR9aTlceG3tMdxtAatIrXyrHosUVTu5NakjQEyQbZudaiFunFp1G-R9T6dZG1iNjDWt1FRMEIJtzaVfrPMwkOQPPM_-HzY0dhlJDrLHGahN2SiFcTrIuDhiCWhJmYZua6J4Ye_w4YmoPBe6T5mVzPIXXIz8o_4mA26NBDz8bGiuS-vSRH-MUPZDmbP_QwJeQ-mLXzpoCfe_UQ33k8jb6leICy42hrbwb5AZ4326UhUG-aW08ujTsZ_Kqq3pkz-8hhi11p5dDKGXL03TDwz-GPgfQDEnSrqRpLNhPYAWMf9KefSDxs_XGY5ypBxEjrJerYPHYFbA8-Dg2kcZTISzWxcDkmEqw42hlkdLfy_aHW7BX3pgyyWvDjsrYcgcnP2dBv4Ni-Ytf0KlwKsqKHInCkroexWtOoymoVWf9W6Xz2boO4qU--Rb_P1O1JQqiXoBwY-Q5-D2LOSQGOmsh1_VkZDPLuWFef6NHDu63lxKDtFpFUGs607j8tEmA3jFukw6DGPp6rET81YonqrJ5mPsq3XlGu6sWXme3Z8cW9Df6ldauCYXGw2m1w4j-tQzSy3iZCfTKqjrVC0EeZFBqvYPmINhhdgo9XPg6bXirT8QbiGe2bWJNypYxx0rfvcdqzFDvBV0boXAZitXkSxTSXel0M2brUY5XtoA2P1ZCvBwQ2qto-RWrISRn389foZHX1BVa9JALftw_WqLdzbTSIu-k08y1nt8KXioQsCIcEOytpaljvb0OnsOPhnmti3Xc37gDwvYrE9ynABFdyCr05EfKyvkQryQy99Txg1jxARn5VKTy2t86EG9tONEE4iey65n0fiGwFnV2Hc7ehEEZ5G2ATKX87y6SxdghkbJdhX9FY5blOdywkkeyBi0Zg-jRmvqcH8MxbGH_STi2vJLlLWylcNqP5cGs_1xnpRbGJMHsUdN92Ct4ytYW27-kxkzNV8J9E-3DA0Ab31iI9BrY4Kpmp1XlfbGIF_BQtoGrouQOA84RAMJd6Xa22Tv6gRuYYvYy2Do_ppEIhXHMJi5zB=w1920-h865">  
</div>  

- Rank  
  - 공간(span)의 차원  
  - 아래는 3개의 행벡터로 이루어져 있지만 하나의 벡터(보라)가 다른 두 벡터(초록, 파랑)의 span에 종속되어 있어서 결과적으로 rank는 2, 곧 3개의 벡터가 2차원 공간을 이루는 모습
  - 빨강, 초록, 파랑의 평면 삼각형을 이룸
<div style="display:block; margin:auto; width:500px">
  <img alt="image" src = "https://lh3.googleusercontent.com/fife/AAbDypCke6CDnEALAPs_LkrL3cs91GgrP9IwqYimUYM30cfR_2MpXjxeagEa4GG70g7j3gLHBnu9tsrkb_CiH65kku1igsYuXKvG96PalFeB-Mnqgr66EIWsrgO0GzhLLjKKnNlnfje7z3Di1eZtjYL0yNAbZQkxInbHKzCrCx8vTQS1fXEmy1kGgEod2nZpILUpTRL689ySZdqxkE4gyHoMUHDgxvEuXXar_bANRW581nN8S61A9k4Przi-9EyNKSpT0PdWqFa2yUpjKg8BT1IPTpnLGvz4qeSmcN7yNyPHcmyqZEUMEUK695vxSBSVDOu78ZNueMp0f1JnI9TNddjXCggAas_aqWEgGP7ap5v5fGVrr1vJD75C-uE12uV23cdCNzB6ReYnlz7gaHjKOZioRh8nE30D2vG_T5tMjIylzL5ZAEMX2vdGypFieob7GTZ_mln-hOR7ygZjfdTM8Gv5pla0KrihPNNzV0r5zoysLYY8LLRBjuxLKbkeSo-8ZfLb1im-LZN-7wA5y5J06Ahd8oRqxZclG26-Pwp1M9j9SBeEScJycQoQgs3NggRuUq2-yIy1ZdK858GaZvwwk2zMa6NY-uq_IcJlLkWoZHKcBeZnACeZoaywDD-7CK03zx3Swk9_XmbbPrR5y2IqKBr9UtnSiiK277tKjgkzRv2geCWcJlTgiIAH3513I4hizrUtNeten_JkY91zRtOXYHxtB4zaVqUEpGP-J9cdk4bK_PktScgVbmYqOgDDNVE3tiTiPPfWUDFwkfVjnA5Kgdnn-d6EIcewzISyokWXFZJSrjaE0VT_7TefVL3sVATsUARu2IkqU4QokOjcq5HNlF6_t00GhRk6wWNMQ1ERjunJGc_J9y8THsIWWlH4PeKpZel6-sYHYHFgcTxY9tPB0t3KB6HLvihiEX4-O3wDVpDT9jpHX7zVMs9UOJXUllJfdrowLosPsMR8yUZyWF8l5WMCzDgaYHEtoerBxWF8LBnFgI-cdlEfUiGEuMsMXikEjlrFzBoFRperexOw7qYGiMHeXsVZDm2fWbYIMOZhfac22rN_QebE6vGMTNsD5_z-LbfKRLr8Jgasjm1hOkr6XQaYp9LzgXAw2n3cIMiIg9K2Rhzq8sJwGggDOUFeAZQgqtZytyRcFmPQloSaZsvYJYkp3V4BqRbeh2PLj8hB277fCUtC566Njx9HcnWr_Lx2hwoVVKLWtPdIG55fStos3CN9x4lK2A0GtJEA2zTwdOvYrJGMYv-lZzyNv9UmRYYqQ-tC1UL3eJDbRY_1DrdkQt2jyWGwHzIcIl1SskLzg3vslHmDtw5zaiARh7MOnKoQ19HYI1p6Ih8-bumlLTWtu_cVRLDXbzj_ktsh2xswdsvoyb3gTQ_sob4wVgUwR43Ua4iCAu6dPxmLAJpdOSKbQtkXQz-aVUbDBWqzO1-xtk3N_YlI6bCiAkQQK3R1HSFA8Q1cWRtRPnf0Djkzi7KCmiLC2c19q2wf1tXgK-q_vBWqMB1Tf2kMYoRlKU9mYSfe=w1263-h865">  
</div>
  



### PCA

#### 공분산과 상관계수
- 분산(Variance)
    - 데이터가 흩어져 있는 정도를 하나의 값으로 나타낸 것  
    - 데이터가 서로 멀리 떨어져 있을수록 분산의 값이 커짐  
    - 편차 제곱의 평균  
  - > $\sigma^2 = \frac{\sum{(X_{i} - \overline{X})^{2}} }{N} $ where $\ $ $X_i$:관측값 , $\bar{X}$:평균 , $N$: 관측값 개수  

  - [df.var()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.var.html) 또는 [`np.var()`](https://numpy.org/doc/stable/reference/generated/numpy.var.html)  
  - 편차 = (관측값) - (평균)  
  - 편차의 합은 항상  0 이기 때문에 편차의 평균도 항상  0  

- 공분산(Covariance)  
  - 공분산의 값이 크다면 두 변수 간의 연관성이 크다고 해석할 수 있다!  
  - 하지만 분산은 데이터의 스케일에 영향을 많이 받기 때문에 값이 크다고해서 무조건 연관성이 크다고 할 수 없다.  
  - <mark>연관성이 적더라도 큰 스케일을 가지고 있다면 연관성이 높지만 작은 스케일을 가진 변수들에 비해서 높은 공분산</mark> 값을 가지게 된다.

- 분산-공분산 행렬(variance-covariance matrix)
  - 모든 변수에 대하여 분산과 공분산 값을 나타내는 정사각 행렬
    - 주 대각선 성분은 자기 자신의 분산 값
    - 주 대각선 이외의 성분은 가능한 두 변수의 공분산 값
  - [`df.cov()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.cov.html) 또는 [`np.cov()`](https://numpy.org/doc/stable/reference/generated/numpy.cov.html)  

- 상관계수(Correlation coefficient)
  - 공분산을 두 변수의 표준편차로 나눠준 값
  > $r_{x, y} = \frac{cov(X,Y)}{\sigma_{X}\sigma_{Y}}$
  - 공분산의 스케일을 조정하는 효과
  - 변수의 스케일에 영향을 받지 않음
  - -1에서 1 사이의 값을 가짐
  - 상관계수가 1이라는 것은 한 변수가 다른 변수에 대해서 완벽한 양의 선형 관계를 갖고 있다는 것을 의미

  - [df.corr()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html) 또는 [np.corrcoef()](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html)  

  - numpy를 사용하는 경우 df를 transpose 한 이유 [[공식문서 numpy.corrcoef]](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html)
    - numpy.corrcoef(x, y=None, rowvar=True, bias=<no value>, ddof=<no value>, *, dtype=None) 
      - Parameters
        - rowvarbool, optional
          - If rowvar is True (default), then each row represents a variable, with observations in the columns. Otherwise, the relationship is transposed: each column represents a variable, while the rows contain observations.
            - rowvar 파라미터는 기본값이 `True`로 되어있음
            - 곧, 각 행이 변수, 열이 관측값이라는 뜻
            - 반대(`False`)이면 전치(transpose) 됨

#### 선형대수(Linear Algebra)
- Vector Transformation
    - 벡터의 변환은 벡터와 행렬  T 의 곱
    - np.matmul(메트릭스, 벡터)  
- Eigenstuff
    - Eigenstuff는 np.linalg.eig()을 사용하여 구할 수 있음
    - np.linalg.eig() 으로 만들어진 eigenstuff는 2개의 배열을 갖음
    - 첫번째 배열은 Eigenvalue, 두번째 배열은 Eigenvector
    - Eigenvalue 의 인덱스, Eigenvector 의 column이 쌍을 이루는 eigenstuff 가 됨
    - eigenstuff 는 주어진 메트릭스의 column 갯수 만큼 생성 됨
- 고유벡터 Eigenvector
    - 주어진 transformation에 의해서 크기만 변하고 방향은 변하지 않는 벡터
    - 정방행렬 A에 대하여 Ax = λx  (상수 λ) 가 성립하는 0이 아닌 벡터 x가 존재할 때 상수 λ 를 행렬 A의 고유값 (eigenvalue), x 를 이에 대응하는 고유벡터 (eigenvector) 라고 함
    - Eigenvector 는 크기가 1인 단위벡터
- 고유값 Eigenvalue
    - Eigenvector의 변화한 크기 값


#### 주성분 분석(Principal Component Analysis, PCA)  
- 주성분분석
  - 원래 데이터의 정보(분산)를 최대한 보존하는 새로운 축을 찾고, 그 축에 데이터를 사영(Linear Projection)하여 고차원의 데이터를 저차원으로 변환하는 기법
  - 주성분(PC)은 기존 데이터의 분산을 최대한 보존하도록 데이터를 projection 하는 축
  - PC의 단위벡터는 데이터의 공분산 행렬에 대한 eigenvector
  - eigenvector에 projection한 데이터의 분산이 eigenvalue
  - Feature Extraction 방식 중 하나
  - 기존 변수의 선형결합으로 새로운 변수(PC)를 생성
- PCA가 필요한 이유
  - 고차원의 문제(The Curse of Dimensionality; 차원의 저주)
    - sns.pairplot()
  - 비효율성, Overfitting(과적합)
  - 차원 축소(Dimensionality Reduction)
  - Feature selection
    - 데이터셋에서 덜 중요한 feature는 제거하는 방법
    - 분석 목적에 적합한 소수의 기존 feature를 선택
    - feature의 해석이 쉽다는 장점 
    - feature 간 연관성을 직접 고려해야
  - Feature extraction
    - 기존 feature를 조합하여 사용
    - feature 간 상관관계를 고려하여 조합
    - 새로운 feature가 추출(생성)  
    - 원래 변수들의 선형결합으로 이루어짐
    ex) 집값 데이터셋에서 `집 면적`, `방 개수`, `화장실 개수`를 나타내는 변수를 조합하여 `크기`라는 하나의 변수를 사용 
    - feature의 해석이 어려움 
    - feature 수를 많이 줄일 수 있음   



#### PCA 순서
- EDA
- 데이터 표준화
- 공분산 행렬 구하기
- Eigensruff 구하기
- Projection Data to Eigenvector

#### sklearn 을 활용한 PCA

#### Scree Plots
- scree plots을 활용해 주성분의 갯수 결정
    - 아래 그림에서 주성분이 두개가 되었을 때 누적분산이 88.2%가 되어 적절한 값으로 보임
    - 일반적으로 누적분산이 70~80% 이상이면 적절하다고 여겨짐

<div style="display:block; margin:auto; width:500px">
  <img alt="image" src = "https://lh3.googleusercontent.com/fife/AAbDypDDtsYOW_DSxaH_uU9dwMd--MSrNihFHUIRBHmRrLaSAvWa4yVvoXlc8LDNwwnGokQtUwl3whZlMrIxU5PygPz0tj9XaQ0WWzxqCAyq9BmPsufHkX_2BZtAiNtRUchoYsDukgL446Ou5JbgudLV6gDMcSqCeUdjup-0JswX5fR6KIshu0r0cp4K_76tpWmL6QAUUVdLowIrpzgWBoXKmoGN5oLOF4h_eA1orqswFwYGZhiDvoZqrXEBEXFmmB_lx5sAjTP-Npul3uohUCtEH9m8po8EPIjyKKjPDBgWgKu_5iVuh8-BKM0yWR1UkHHSCcPJaLLjUMnvKxvfmDkpzYf9L-7z6-R-UtE8q2CXHbp2LZ65Blj5r92AlLSbX17VoIcCjlNt5GWRCmHAnCXqJR6nZ9WBh3ajcUp59Ocdhz8aLnfDRqPqJOJH8O8mjy3wadFdzs8hAjj1XzR0Od1EJuRiFF4K-9niIbw2JlSH2xeoVQKZmJhb_sq9DIcj-jNJvhVxXD5zLMNgU-MqTEuWok9wBxwPcEg_JMoeOu0-8OWoxJrPfVcy3j0WpoDv34JtZE_CWq0FAzQvEHz_6pxOJP51CCnUdRMeHQ2H2B8A8IAJXeTk2h9qbtVav70dDpGYiZnrvSEKrrEJFCy1_XqisQf-wslbru7v1GpsznYCZIQeHOPqZoJ4e19ce6yFESd8kowF2sH18b6b3jiQzyITL6aoeAvDYmyNusp4f-lZa1bN-dUMkiOcTHmaTpIOP7zedoXONTYKwGr4LIkioABNh6yDPK_H5ObaukoRYU12xXtW9OE2P3cVK-BOOBBxcy72Oc-a3P3S4_90394NpEjLpkGMMo4N7C8ZIMKU4jralVksvB0T12tGxjS2bvDYpQHrv8_G_CnvQeuKALd4AWINkkEgaVtkZ9mhXMG9-bdu56hC23XOttx1-gkPbCs5a_wgVaN-iSM4g_Gica2twSRfna_xrwxcDPyBb6UGI-T3ZiQdcNcxIGG9XDzvaHbqFGyjFBrZIrRtKkW3_nDcmRHY5rd037B8wzC1MrFEjCdp1ZAix-eLNw1sa16h6l6zTY4f4jgWJKo54YQEJcJ1YlvTOvgT7j9ioElBnj6MHZUKLhdoqgsj8EmRx5RyJUbbAr5MfSyvkhEPqxh7um1WHwqKBHbpTiWvrI1spFfhJSPiW-G2PsTigxuseIlHsP_oTgP8CZN2lMN5CR_EtAtz9UI5AkdaSaKmNMPwcliNh5TGjMoOkMWu76IT19Kk4-Pd6ffHHsAwtiVYH9qbli0J3bgXEjuzsu81MH4SXzcnTKPMoX1ZkAZZ_vP2YjB2SoQ8UwuFoh7Ke71kzVVgBGEEOyQLloSXChCFs3YwiT_1c1cvmgX5CFPs7M7q-up0FrFVh89JKJClFaZAKjydYFCaEbCjeE2gRNbTSnjtPn3XDyhFfJL2isycAZNZmYZ53FMrJn1-NUtEc5IgdAku9I1NaTorxtL1Fz1zTSijhb9ianJVZEp2WCfl8RDuUrPJk_Xx=w1920-h865">  
</div>


- 두 개의 주성분으로 만든 산점도
    - X축은 첫번째 주성분, Y축은 두번째 주성분

<div style="display:block; margin:auto; width:500px">
  <img alt="image" src = "https://lh3.googleusercontent.com/fife/AAbDypDUTL3AWH_S8I7TS_23H-5QkfK2CWN3tVFRA4oVPIhN8OGxwBkaem0YLNyEHYoDLYLAoPUhc8pBQXBR6-MawcSZa2fDPez9WT4Ml675SIKhNBGqmwdutgqL3RRDQY6gyTm4lp0zhGHb3Q0Bh2ulOcyNcq9a_W52JSfmzWiSXVe1vX4_RqOQI0BwOo-9yIVvtWhyUlBMoGkQ41F01Edyrqs_3Fd4iIL0PTEtM-LSStZ4kVI4iZrRIJ9Y0pwolmPUJKDPz5dZbNIs618S_xWu5tEbQqbxoy6d1157eFZ45OdWdp6QDX5fwFg__qFR-dRf449Z9zmXWL3wgfsuigQjIxpkts4AD_sj918OUqY8n3wXnJVgFA0eK6wMYvgQcxK1z-3J0aEevLnI6F-deVqqur6M5G7_hREJHhHjDq9dAl0klBQzsT6hgkk4dEPDmt6wID4TEzrYoRBciZIiF6gMgeaMYWMAQ2nYBgVoxE0LhmhGLqjT0zZq4gsBLW-MSZ_Da5g0UoFRRGflNWWv6kctH5ZD1Ic-KnrxdGNJKFtBbPGQ91SIlEcQYrADoWljcziXmaF3XfULLCvd-l6DXgZzPNoaDS0V-xuT-7C_yfMOJgoIQQ2wwYzwTvHLitEEC1ncgEycwRU4NHSC-T_TDt3M-HsBN_SNNjrgWmijpN2YyjoArmdG_gl531GH7DUrhGYhU908Ayzrl_kT5tvK3i6Y_KG2OxeegFyq5mhuw5cOvC_BLUt7TV6mK8LAMgjV5S_G-WK3fgITtfRcTpVVPMC1rUKpRFrvNlf67drfyRiq9iXQ1uiZP-0jThBmo6QjeMPpClW_eSmHKWjpSSYb4Ea-S0WIoSEUdhjTGwbpsxg5R0Sv3KGw6YMOaIGqXzowEJ2Nxs2Tt6-0ImucvyC5sYjZaZpRAmOzPchW1AFe2jLOnt5ScHemj1Wg15La19JheeIWaSs_aKSD_7CNIfrPOJEeuJQHs95zEV0JN3_4yG6F_1x6pHSTnQEStRdzmjlPzKGOL0TMLzZMYwLzVBjjB6HwjGO1rCLZaxqWM0bt-UhG_VDTAf8apjaw-zMDh3E2pqKID7BQzG0FOoN24XaNA0ZzGBmxZRFQcLlwbIU56BXabaxbPT9K23R_qX5RcmifSh8Evawm7GZdZFXPaUUVj4xk9uWrr8WnPgBQE6kwd1qLw9OTl77bAm86OjfNvK7Eo4BNbxw5RGbG_lIr7YE_AeSYFuE-t51WK3Pq0-MKDYjBp_HiTA01-bkjkHZcNSiNbMBLJ3U3f4roSbmcDub_HJik27K27EFUYULsp5UXSc9yuJO1BVzxg3ebWf2Ecm3E-H2p1RAAshsvAdTX35EupCxY-RBaXo4mU9j52AUqZYYSIxqP3wQpJbF_Muqu7s6pbOOenbB2VTZNVDniIpiJHe9CA1j5dT7Ed_DjaFFltFsoZWq2yzUpWIp0viU3R6zn9fcfEX0It6u27ckXBYi8g_bpi4tgSTwoFJGuf5_mvSV6ovPAFGkwt1eOO7KsI4Jz=w1920-h865">  
</div>


- 세 개의 주성분으로 만든 산점도
    - 첫 번째 주성분(PC1), 두 번째 주성분(PC2), 세 번째 주성분(PC3)
    - 세 번째 주성분은, 2D로 표현한 좌표평면의 산점도보다, chinstrap 펭귄의 위치를 더욱 입체적으로 구분할 수 있게 해준다.

<div style="display:block; margin:auto; width:600px">
  <img alt="image" src = "https://lh3.googleusercontent.com/fife/AAbDypAq4gHvzsCCBO0b8z2aSJwvx_NpoN7aRemvkjpEKwgp8S8Kz9fokppi-X9RFZmF_crHR1y-xQ5XHCQtcpGTyCamt4PeJxReBHnFdBeFVROGfsPHi9IcySIh2rkEfHwaSCZZNXiV13slqhSN_OdQPA54fHYSb9Dw-ISNu6Lpa07jss9S8cYPiHvT6yvcc4S32g7-C-i-DcoStcSptae_MUZYlnggM-NZ1n9w40ztiVsY1NZKxaby8M-zAuzmwsQT57auVQoiV2_UKgAhoIcQ36m3Aw3bHhBRamAschtbM0STIm28qWrMGsR6jAk4skxX4EeJdNmYsi1oSiE4Qc-t5ul6EBWQ9vAFAAJRr5diQEESGSz2ujrKmQFd9CLsaZzaCM63SnHBxYcjeYyZPa2R32irel0tguiMxodRbx4iHLqaOhc7iFrtoaPCIuj5ZltvXKcbbK7RCRIGri7ynzFMp--Txao7c0J_mJJNaoK3ERXigMVhMM4byaxvh3KXRIj42k8huWms1zTlX6ub4ykpshdAHiZ4-x7Vxc7zmn68YVDLWJjvkxHRPFhmmoVlH6pij-igc3Q5BsPE-PW1cQUz_XQmqxoC7eulJTMF3mzrdoQ4ox6bbjMKxoniS5F9kJBcUD_3ZwA6rcbuAVkJt_fbI6CwF344Z31s0PyhNgARm2tehPaRgCDqTU3HmoPFsDO6PjArJOz3i_Q6Mlo_AOr907qx-aqc5DMsqaL0_kLu1zCFjkcRZyRSdk_U8a5a9R-ejbhmvLWcNZdyCA_YAmu4e3xTcR3Wm3c-r1mCNe-4bugW6Yaln-TeaX-s40Gjlcd3ABUAxDngAaH8bX22Unqt7MNdjMKyBIL2wJJ0TxwD00fB0XZTT6_mqa1n-C2aWRjumagcynEU0ZHqa3Cu1OYt1Ry8zSv0ZbdtsTcXQhxNuXlHculRN0rg8wRyaikHY80ldG7Yz6wp-eehD-yXhslAcXiyPqLbeS6eodorJcbqKgmdPgp_fOlOHPkWU6DwEdPdJYK1yK6ewaajisIozJAvf-g_EPJavGxvDKK28S6Q8oU3APxd43YWy94m8GLlZMaXNxQWUGL0n3bo8NnUI2gfyZN_yZFgNC81sjpChNN2N-FrN_2mekaMd3Gtp4POeOhzh84XUl_gVMZ3NboGI7aFJJGxdFRSGxB_i56a8oV9Ii5faKbJqb411_Q85beirbTD1bHv1Hx3qkHxd6siO6dnpHSysEhYzKXoU4Ueh4oZLVqUU7F6RJHtmWLeu3tXQq-g1Pa5BBI54r148N-DY-pURE1S1Hp9Acir6XR_1Gmkn2g4Re5t9soTKbB9lpka02MhhEX2A4otVlPII3132NFd4uJh5pffp1SdfYcPEcNZRIIlR6-gxBBPa6Ud9iJ_1hX16wUZUWrCjHjR8XqyMHWrdkx9d2p-ArUGk5CNzT9z4RF7Dp3Jnjwblx3gCsP8wAbhNzpJTMVWnKp4TGlKah40zERuaDGhUkG2YPwFq_EiuxaPJ7T9Tsg2AZU0rca1=w1263-h865">  
</div>




### Clustering

- 데이터 사이의 거리 계산하는 방법들
   - [Euclidean Distance](https://www.cuemath.com/euclidean-distance-formula/)
   - [Cosine Similarity](https://www.learndatasci.com/glossary/cosine-similarity/)
   - [Jaccard Distance](https://www.statology.org/jaccard-similarity/)

- Hierarchical Clustering Dendrogram
<div style="display:block; margin:auto; width:600px">
  <img alt="image" src = "https://lh3.googleusercontent.com/fife/AAbDypBaIeu5tvgPWQbwdNgIA6qGF25D3HjD429nUeSBWSxzulcM13T0Lx1cDVr4FpJ8-6qyR_SI5qHmkb_Pw9Ti-DS5LPNBfJiah4Yd4h6PIQgnZMZdwLG3JcuuqDTwPVmcD-sUGS_fUUm6V1t4b6b5IrkAAZP2tPloufORqhZS5Mk-nf2Z424Whm8Pp-fqBZWioUShqPPut9vlHwojdF7jJ413mIGewJYHed7W5f6FAl57QZa9MmJl4SCLpxmzzwd8Jls1JHnZwosQComkpQwCfKf6Z2k6NF3vVRWYhiLJ-5nh2Mt6gbaPthPYPzPTYjHCW3HJHhTwI8AIMQmJp5Awk0dIBfuYJnbCLYTtLRhzf5nWZPvqDmNw-ZbzBcmKajB5GqK58O7S6BLyDNY2WhuoTKog4g8jVEc6A32_HLAd2KhXjOZaUcfFnlJQ6JDZ_B0ZM3sA4x97-qAXIOnZOjYwUDSYqM_rQgplmAn7l8TgnaokCh6AQrP4hKt5FcAJb0yeqKVd_ebb3cbSpRt08ha1UPhaXZj7O5H31eOt05eoKzl0OMn-t0znLSTfyIyx_uIvw3A19Z133YyPnsjasLuCvciISDTTVV6hW77LMYKCTD44LI72O90LhbaAKi7PGcpixsPvNztqOJdFETqaMad-DeprNB2hcf_mKPFgLZflSK6ul3MrzTD-BvXdlGi7xkv-xckGKwtlYK9oSoSzRhEoky_D8uz55A23InJJTCBo6Ol0M8LwFZYsKH_6ebVCL1uGnTEGt29yMNpGqHQqJRiCyXxSvIRBdN_tTc9ZfzbWYTQhWu4tRlfC-IToMaA4PJ9mmKWNf-CVUIU8Jdhgzj6PiUAT5r8Cd20Dp9U_mh7bczcC2ioZyjnRdJoCsnX6Wx5kfZLkKWnhefJvPutDQHyqR4TPGcD2092M3-rnh58APsmb4I2OrLin2AdYk5hKItQdOevykOES7gXlW4VgoV7NeXLPAXOPC91snp46ipB4aunTvrR83tXbWYk-a_aSB8YhWLFX-SUpHSIGeH0RqfOz_MoBeD9eHb6xEjiVy1NF13PWonV_0n3NXKt66TTopSvY9bDBxGZf42HivGfjS5-WZruYqK7OJmKa_9Dbph5lG6cLYjVSWTQOn8EdpdSu4GPEvR59zVdDjTmJwj5V0pqFipfS0EwckToNewO7LLKBa0npAqhtbw3ORnSVo9uPr9A28QuG-6vXbmESMvXih1_-BYJgWECIWZ_O4r7raJxn2DOw4pWGEChTnY9stUcgXUGGzD9knLDs6umIMuQL07CvcbhEjYauKB6HzO-3hIoZhF0HCPkDF2NqR8QERIn0bDd2-duh9e3cCc4CcdjRnCG7Z2OYpNyM2JGUj4-wFSyEue6lZ4iDOMxhVOOVhLzCNTCIb4cNik21bG6OGLt592iLLQ1ZDD6vSqdrTUJuJy9bL1XHAMR-TLS_7npx5iQKbibpuEO8IXdKV7CxB6Z7dSD82ONgCHYvsg9IuhtlStNRMpcmyza7tDO45G87hwT0=w1263-h865">  
</div> 
   
- 실제자료, K-Means Clustering, Hierarchical Clustering 비교
<div style="display:block; margin:auto; width:1000px">
  <img alt="image" src = "https://lh3.googleusercontent.com/fife/AAbDypC2XQY-4qk_l9al5TPHbjY8luFwuyb0QPaIyNa76tUv86uyLfSAYXHho_JmpYjTT_lT4nTtDNIPogDKrT2nCDFNCBfSbw3gPT7eO8tOBNJAWFhTmbvlLKwBwySZuI9ex7bl4fOFJNa1ha9LIRhCPXCLwiJ7skRDcQDqxn0NyoC3oko59jm1WKy5ucXlQ8X9EaTcg_pNSOnhC48uSMnFbnYyPMyV7MDPDoR9l10aNnd7DCap0vKlkpqRzSso5Ol3vjiRw-DT1MYmsntRPfkhLxR9V-h5r8MrO2zSfbRq3b5lRoZc90kfPWszxBwK-T_Ztg3jPKZ2poxvcx-rRKbxdFIkMmUtl0799iS72QbsZ-mVZ4Qlq1p31oBsIQBS2GuvgLG6w95Tekzj9sn0TTclUV-FSx_tM5Xz5euN98gKtNty__PsPDxvMXa1Y6dlduWkIYRgFslO7peOAPTdNRB3amjV6J8DXJ_2S-zD8V1E67zxh5Ck8qG-TlD579oKtTqm522UTJuHCxnKDuKTQBOzYfk8ROO5B3xLZHSKgZet8dEdfMl-b33chBGJahY_nK8-zThODt8ZxdHow3cB_4mh9eQPH6ABWCqwcHxr4R5TQwLqqeXgYvE4oybRF-K6v9NtV6Lb0MyR0Db3kbysKncfhHVWDwrv0U9vVLLNnliwZDFhZ1N-yQHUeEZuV_Q_IolIGC93MCjYWYzr0n6q3c93WawjnSG9zmadtOc_BxWtPxnwk0lQipNFLBI1FlJgCph47Vwja5V9ejI_YgY3wVUid2M5kQ-4m8t9UCZLeMaCD3cAlWAu4CjqPynOtqMl28d9l4ni88er9n015v1aa25v1uaKkVn8sVSaCwpA0TWtpZtZgKXBEiJUlUe3z3W0xyTzUM2-gobsuk-QGQzf8r7xD8O_oO_E2rZ5O6aslJgb7sJy5kluRqoOKZXDKLGos_xOsGfJKWUZrpKNge82EFVt8FlISfrrL5xa0qhICx3Yu78azFul8uiIIEH36q8Sp5uEf63NgrgSs_-FGj5n8Dw-khg9dwvSpLfhi-3R_6cj8hW6xiRWUgvkEiZ1KqRtIJl7BYkT099kYe6S_2aL1b72OfuTbCBPvt6PLRPrTApNfp05-uzUgWjlGacLMFtDNsti36P-1K76AMmhEEGjKFb6LvChEjHDq5kLFe1FiNeImd8fKbpY3cKaYHWJ6kUYQxDBJCwUj38qg4xEpywbQmUQzwh1hlOTxEQOW0SBlHtxH9ZFv2UQ0pEZZ_W6_YDx1GfELsPXsdqJaei-vydPlGkeGZIDYv_aIQ8JoRpItxejwJpFMHGaojCQGenUw3yZNEowSJR9NAulaxUdGPUEOQFw87_zhSrIQZZsoelgA1PsbAX_ZaF5k0Wrl2Napxdzrq78lznftifTE9jeCjl3ytVPI7Y2ya00mhUlg1jZ2fFGglRPU8lqpZ3qCGfTsu7WCMnuOyWbciabol4qbX4l6IpG716Gu4u0UehPbEcxPCYB3jj_w6LeyVOIsO41OIsA=w1920-h865">  
</div>

- diagnosis - K Means cluser 결합 비율      = 93.5%
- diagnosis - Hierarchical cluser 결합 비율 = 91.9%
- K-Means Clustering이 근소하게 결합률이 높음

### Gradient Descent


- 일반적인 미분 공식  
$$f'(x) = {f(x + \Delta x) - f(x) \over \Delta x}, \Delta x \rightarrow 0$$

- $\Delta x$는 0이 올 수 없기 때문에 0에 매우 근접한 값을 사용하게 됨
- 보통 $\Delta x$는 $1e-5$가 자주 사용됨
- 이렇게 $\Delta x$로 0에 근접한 수($1e-5$)를 사용하는 경우 이를 `numerical method`라고 표현함
- `numerical method`에서는 조금 더 정확한 측정을 위해 다음과 같이 계산
$$f'(x) = {f(x + \Delta x) - f(x - \Delta x) \over 2\Delta x}$$

##### 미분공식
###### 기본 미분공식
-  $y = c$ (c는 상수) $ \rightarrow $ $  y' = 0$  
-  $y = x^n$ (n은 자연수) $ \Rightarrow $ $  y' = nx^{n-1}$  
-  $y = cf(x)$ (c는 상수) $ \Rightarrow $ $  y' = cf'(x)$  
-  $y = f(x) \pm g(x)$ $ \Rightarrow$ $  y' = f(x) \pm g(x)$ (복부호 동순)  
-  $y = f(x)g(x)$ $ \Rightarrow$ $  y' = f'(x)g(x) + f(x)g'(x)$ (곱셈법칙)  
-  $y = f(x)g(x)h(x)$ $ \Rightarrow$ $  y' = f'(x)g(x)h(x) + f(x)g'(x)h(x) + f(x)g(x)h'(x)$  
-  $y = f(g(x))$ $ \Rightarrow$ $  y' = f'(x)g'(x)$ (연쇄법칙)  

- 지수함수  
  - $y = e^x $ $\Rightarrow$ $  y' = e^x$  
  - $y = a^x $ $\Rightarrow$ $  y' = a^x \ln a$ $(단, a>0,\ a \neq 1)$  
- 로그함수  
  - $y = \log{x}$ $\Rightarrow$ $y' = {{1} \over {x}} $  

###### 선형조합법칙

- $y = cf(x) + cg(x)  ⇒  y' = cf'(x) + cg'(x)$

###### 곱셈법칙

- $y = f(x)g(x)  ⇒  y' = f(x)g'(x) + f'(x)g(x)$

- 예시 : $y = xe^x  ⇒  y' = xe^x + e^x$

###### 연쇄법칙 chain rule
- $y = f(g(x))$ $ \Rightarrow$ $  y' = f'(g)g'(x)$
- 예 : 정규분포의 확률밀도함수
  - $y = \exp {{(x - \mu)}^2 \over {\sigma}^2}$
  - 위 함수는 다음과 같은 구조임
    - $y = f(g(h(x)))$
    - $f(g) = \exp(g)$
    - $g(h) = {h^2 \over {\sigma}^2}$
    - $h(x) = x - \mu$
  - 연쇄법칙을 적용하면 다음과 같음
    - $y' = f'(g)\cdot g'(h)\cdot h'(x)$
    - $f'(g) = \exp(g) = \exp {{(x - \mu)}^2 \over {\sigma}^2}$
    - $g'(h) = {2h \over {\sigma}^2} = {2(x - \mu) \over {\sigma}^2}$
    - $h'(x) = 1$
    - $y' = \exp {{(x - \mu)}^2 \over {\sigma}^2}\cdot {2(x - \mu) \over {\sigma}^2}\cdot 1$

- 예 : 로그함수
  - $y = \log(x^2 - 3k)$
    - $y' = {1 \over (x^2 - 3k)} \cdot 2x = {2x \over (x^2 - 3k)}$

##### 2차 도함수
- 도함수를 한 번 더 미분하여 만들어진 함수를 **2차 도함수(second derivative)**라고 한다.

##### 편미분
- 만약 함수가 둘 이상의 독립변수를 가지는 다변수 함수인 경우에도 미분 즉, 기울기는 하나의 변수에 대해서만 구할 수 있다. 이를 **편미분(partial differentiation)**이라고 한다.
- 편미분을 하는 방법은 변수가 하나인 함수의 미분과 같다. 다만 어떤 하나의 독립 변수에 대해 미분할 때는 다른 독립 변수를 상수로 생각하면 된다.

##### 테일러 전개
- 함수의 기울기(1차 미분값)를 알고 있다면 함수의 모양을 다음처럼 근사화할 수 있다. x0는 함수값과 기울기를 구하는 x 위치이며 사용자가 마음대로 설정할 수 있다.  
  - $f(x) \approx f(x_0) + {df(x_0) \over dx}(x - x_0)$
- 이를 **테일러 전개(Taylor expansion)**라고 한다. 다변수 함수의 경우에는 다음처럼 테일러 전개를 한다.
  - $f(x,y) \approx f(x_0,y_0) + {\partial df(x_0,y_0) \over \partial dx}(x - x_0) + {\partial df(x_0,y_0) \over \partial dx}(y - y_0)$

##### 미분결과 시각화
- $f(x) = e^x$ 의 그래프  

<div style="display:block; margin:auto; width:500px">
  <img alt="image" src = "https://lh3.googleusercontent.com/fife/AAbDypCbvh_PtfNfWIGMoooX2ZsemBYht6Hf3youWMciP0d76XZ9BReDajGcHakPsftv5t4EEF69sZ9kG0ToaAT1fuBq6Mv0IHa91ULSb5gcNtGgJDnuI4-EnLbLY8e94NOqg2-8aA0Ymb3XuV-WPJ7mYTYvmDEvg33OdElc8mUH9cLT74n2TG8YbSVxv6CxYZ-4M3bfp-aYe85ZQk9qT02-LHGop0N_uza0Y80ejo4cXddnTQpelkKv4v1_GkSsfqPdgsAAKrygedbyi9Yum7R8CIBhCmjctBYmvLlct7HEAWes98Iri0LmpndPnDuS--XWfbNJqUPBPpzZsUk_JmrECyjsyamT4WAzHROY3KvycwvcF8Pv4_Y6QuoHrYsJ4XfaLevX-2YmtohdSpF1YXbWnCvZxhALHktjMvdlL9m48SVYbNQm2rh_aBJDUYlYMEfKSoScwhzRMqr-jgCm2jEmCLRGIxtoVoUq0TpIButbGmQoLCGWBT9tDCgyEQRORj_UDg2LtRfQnIIgv58jT8X0_tqaf_2ZboZLIpXQTQLN3UH8VsyZKi14zLzjCuLhDCGDqI3pjaz9RHPkeJI-1GcMP94-sx6p_i6ox12rJV6ZGjOe1NAcuZXQYwYCK_ghLJBBMKM70TKxueGS5dUXtA2U27nDPfMaixiQJuuIXTB70Lm0pvnp8g87lDken-rEJAzQUYBxoQ5IhzViPGncsGU_Dc0Rr0Gqa_B-mtGZG_itsYkCeu1yIv7pe-0K6QoRg5kdoWEMvqA3W6dzRFuqgiVCh893bW-ViT9_q3UkAVS6KAvvmgYk4ihCBFJ2I-Ags-YAKz-R1buUD78pPiW7CGJbB8VAwyTSrNi8eU91D_1D2IxNgJmZBn2oswFS-MyK5M1z054uy432AjFahVywghYJq3V1mfmP-wqy0WJGjMR1bmGDGQlJEuMKf4huWJeSXM5zybhQuVDZZ-6AsHS799ogivnR2FFel7pkzfIBz9vyWbs2wuaHm3J11bNOwjEbBDp-29QP6Wo0TvmSLzjiBwzPm28UxlYpvc2JLhy97bqtPZJ4JcYBFIF61I-Ci_P_QJ5xTOiHU3iLeVOH8C2Oggi5JEkO0ndJX2dJETi36u8N-PrjxYUjZfB3vewYZfCZp_G2BvofjNqtsqUwLWLP7gtcTISEIkbyU1yduRYptdqUlbPW5fvZEmttljsd5mbYjSY-sgB1I29pgnutDHBsR4pOLVxZqZBVtqHYuJCmYOa5V0ErjAMqkkwfM1km85fc0mRU0YehIU1cWEyimfd7HWcFxVuUoeKqRrX26s5YXLU8Pi_1o7pd63O5TJvHpG095j_3IbUY4vbAcvyM0ab8kMgx5kIhWsEPcZHXclfT4sq25Oh3zYG3Dl90vr1RugGbStEsdXrtobRsSdSaZNH54JPjeAiBEoW3Eb6LGGH46rUjT_g-CzIlwfrQo0YC-zDE9xYVUIu5_3Wf9YCFKOnpoJ233MIxDNuoONnMurRxe1jzalmXhjxgveHXPM9BNVmM=w1920-h865">  
</div>


- $f(x) = lnx$ 의 그래프  

<div style="display:block; margin:auto; width:500px">
  <img alt="image" src = "https://lh3.googleusercontent.com/fife/AAbDypCw5GXtvssaFnVOUw2_cP38hGFmbBrtj57jcmDb52ygD5br__BdN6O-x8wtGXWmucn5MDiZhun0_5k4LPWJu-_zrz_QSFFylynZb2j_oRkB4ESzYR7y0kshoB2OGH1TRM2-7ULs8vwfVwfToau1TvtaiVMnxg61TBxvfxBxgSGY858HRdsqpz2fRaei-11VQy0ptKr0ZVVBq_Mt36_N2eKJk2eMe9lD__oPWqNrh4jWbcgkY9RRyP5zKnIsfNWZ4OjhJ2DTxRfNFE73qOGOA6uRnBhE8R42OGGPyVxdbNS9irUwd2Mk6A1XcaaVuU7790MnU7Zg55GT5fGufct_XfnxlihMIIik6Dxy0dIqvOcebNdIGlIzFIspozhSYl5RqYbsDV14sNcjWKIkwU05-3f76X68BEUG3D0A-Mh1lQf6HfbH9wUSR7tIOU7LxK22yFnljZuqJhb5duliS-rpyyMuHGzcBoxPbdn54nP7n-mHqJZs_xOMJfaf07VHKlUlsrqQ2ZWFlbt0fsEphBBd_qkEDsnQZZAfFD-nKaHjEJKwlC_sW9HGks79nUb6LdciJH2VRY8esyRrnL6JcT93V7ontJQZQR9hblENmOTVyo0k1wCYiGclohehylY9TFGp0Z5gg0hnRGHqW2jLxFm0mBQNrKR9zVTQ-CidcGiJuLzeRCGwfPXKiW0klydnCijXrdlY8bdM5y_ucDY_L2RED2ifrGpGsUns25WIvdX77yaAhGpEHvnlDaiPRzZ-ceyTqKUZgS9J6D3iJFLGQuJnRp8AQRnUGRBDQZmBdWlPZCeVO8zoOE5euwgvZqohs4O3tkKVV2diK7_o9OLZHKZOxGYr6y_JJt5DoCSxcitUSviq9w1VJXOtT38wL8VFIw6NB3ePJRCt0SM3amM8ZbZmv5GKkyi7k_S5LaXDo_ceMbaU6Dy9-6RpdJbViPb2A5e4xdIMrjwqpzZkppw-uuxRCEphnmPA0qC8FPym-wnl9gzwxhOvbAfE4eTfOQtt7RgHZGpcKe6canmnVf6fGAhpGHGxwlmlOjgaFaCYOQnwp4lAXo_GWVbWYUPg74YLI9NR2BdFmshEyxhKZzjROwetdzxy2-bT8RjqfzTIqko9elQUk0Lq6FJzQv0S22mXRq2gjdVJlLR4ZtXGBBTaA_lh7Fs2RQH1kvlajwq4zYo4rMkEfWBaVLDm6s9LNMG5IqQfGZp3JKWp9-bcwQoIQOMrR73guGJBpVwZiS0Qnz6VPiKAsfTfaPiQGZ8MKpP61lnuTDbIAtzwC-oe1iAB-TuJGYsZdolzjuS9kp1f3o-wVj2yNmTz9cdn7UuftVZcZvSBP_nTLOmZfAda5UuP40VT6x8G_lxaJQ2kg2s9YIo84cGbXDNAXChWW6sWtNMChyVQxifjuvvy79IKDcYRkAkXzwoQSKk2a2h4DLy1idlxCbtiRseWrqRSOZt6VUqAyYRhW15yeuSxN-Cu8ZPzvm0GGtScvtbNGoj9U7PtJYiYF_W0HqfqX0pRC_bmPQOz=w1263-h865">  
</div>

 

- $f(x) = 2x^2 + 8x$ 의 그래프  

<div style="display:block; margin:auto; width:500px">
  <img alt="image" src = "https://lh3.googleusercontent.com/fife/AAbDypB0v3sr2PYBEX3mIcJVammpynZ59d6_oQU0_TIY7DAkog5_e49XQz4Yh5fz58WEvy1DyhO4Va_-ciGLn6GeIw8BpZXoNi7gWm2vvvOTXVivnwv-Cg3o7igeeWbaNpq6FDTQ4aZIKidTmNc5n8rbjKFgvwpGwG1TooL5TV-xlGM3jgxeYYgUmlvn_sHvXTyp97CCmEkttfrzkXMe4XBAlhQ2W_L3HCj0QS4KMOru190UEC_C4A7--Ua1sIHeV5mCF6gle0jbqe5OifmF3jwZ0BItbeqRRGbWpgRnrzXbky_OErG5vkws8H7vRYxkqJQNvDmDbNDMBbP8veY3vVhyc3EDscDGfq5_19imNBFxuKsjH1v19FkWqMjovBhRgh0RP8wmcFKixNX4eLcdDsc24g0mvBCA268XZRTZQOen4IVvXqyb01vbhpHKZriCsK2toW0II4RlWUMgkuJOSb1XSWwn45CTsYSgeVIa-nu9BXrATxKiwfUtAEPnJx5bVedmuWU0JEqYXI3fhEa32fgonFwdYDZonhOjIuVq2TKnJwgZwmP9mF8qwew-PvRxYPo2ARk8sOZL_0kXr3TYe57ozNr9MrefMIiPUNof63PRCjqXM6BWW0-Fihun297VyAFq_KHQMSWvG4h2A0YOq1d-4DQLJR_F1z9XAJ-a-aEUDYJKbTgncA-aPbhHB3xTe-ADjfwrTmFfc3FZ5AJgb8m9iP9-ZA6DUo3tf4Tl72gsZVdiHflRC4a4YCbinIJwD_xpk7qLRyKbd2o62TOkREUA2EGrsR1pahjQulAQdpqlwflo5oJDYBzgOEqb1WkwwlrMXzYNMIhAjtnK5OyqN3UVbEl-YfLYI8PHO7J7X06jw2cDACxpRpIorRjOOGtYmteCtfkPl6TIzc5r7P3mCXOO2GrcIVU69a23XyBjZ8gw5uVxehCfxyBOIWAKfZBy_7jU4_D9BsvvKfnnWgDt8Xm5Vce-uUzZG1bmHezMLwsnLimgFwcOVYMkAAhZL72xrcX0cxNK-01M7q-hfEtKAyPRUOIt3ZM6UTwjEmA9IKMdH0idnFCx_Hj6WJ5VNIvTLgDjr0DZDwkvEtpDswSBX6ryPd1ueTQYklKx3d_lJcaOwH_rWD1t8FjcBZPvZ-_ulkgJwZ8pKSTDHn3b6G6tMLHGEDN4890nv_L9ioH7G5QY89jC1D_3KTK9wAkBpCUGPatHS1edQegl2FFN91N4fBxLM_DLgde-CMiCnSuXnhIylBdRGNWNfzLWeJjzhtGPuL1pbveVGA6_6HkhyQclKASOetlm5X8TTQt_GIZw7S-aQm-LGx-2H_g-bm6Il3bp8tehgEwXv-PQF7B4G3F2Ekhhvq3_yA1XO8nhZFsBbTqUFbZWm0aWE92yD3zdOQsnR5k_mM54KQCoOBlaXBsoiB_cKukwKsBAxvc4aO2o23_4yyM1pR0ZyITl3-IXqLIvjWF6TxsazZf77XO-FAsJXPHDd4LMBZSe9sdGEiV3SBGqQEAjLFL5SmoUBYHmbI31=w1263-h865">  
</div>


- $\sigma(x) = {{1} \over {1 + e^{-x}}}$ 의 그래프  

<div style="display:block; margin:auto; width:500px">
  <img alt="image" src = "https://lh3.googleusercontent.com/fife/AAbDypC3rtGrmoSC6UBzfmfmq-Uo1TWwE0Tupzo8qp3LSmcJ5zSMlEApvPgNRjmwnlMxcS40JUyyj10rjUkvpmlK4DA-M7FObLEfMoeVJs3Bxb-5QNysYn6XmTmh5EX1TwBr1Vk-RQuaW_BcH_d8ogrYzdB2hWpBTIiz80ykqKUIjlEiOxDMyvDhEi8DxNWla0E0RtWO6o6peuXmrm2j95T5O_ckzXPtQOENBSCpRjrxDaH1Vi3as_aAkvhzXxEdd1CICYwOxXjm2l4WOAIhqmXVaF4JwrQwtll9cmiJUlkLb2MvPu3_ZGh8lW86J-U0jK-Bs_Kkh1ea7gaWRQD2IiMgfp5m52_WFuuyEA5ZJx7QLtSgKgH6aMvFv5K4BsDN3ZtT4MKNQnG5Hfr20ofmZXnlhYtR9TUPLxnB_i_4uJ20qCouoFYsfeiC0bK0-azB565vxAYdFQ9WAEGYo4ZCW-gqotqbMYLeoSBt9v8hJquhslE7dcAOGqlISZNM5yqDsYhN6ZidHUvvqgwlGcvLSjz3Iet4toVRk-KhT97jVd8QaxiGp9AlkbEZisbt02xq_SGnW8C_l2iIIDeOMiDc1Ddj1bT3Zl3hUHqwAZO1PNsGMFkEkVNhcYU3mllvW1dPPCqir0IK9CwDHpZahpNSSrCtYltgBjDpZ6yAJUWidmGLrlZS-zGTaodfGQt-JdICGUf5ff7TVB_KBX69VftUodLVPsEYFNKGwAuvvm6fYlCOEojjrIRBMhQ01zpWnUwh-O4WUhFl-RSKTGrt9HMFhU6AlajO4yMESWEvXnvoYlHfszY5qnZEaiAYeg0lOAk-QQqvqq0O7e6tYd_dbxos47vHBlDNC0Sg-BQlh9iQ1f6LkXzH_96T6oxfboTEu-K4GB3en23z955Idvfurdl8biqtqN55JYl7uUdtH5VYTQYq5CCtpMwabRugASu6AYcMYc8EYfrKj9SuWzM75VJryoMRXYb8pjkiuFTNnzZ8Mxj0xoad9pUhmxWAIIXv1aGyFRiQIg1hz_Ks0BS6uonzgwBBfVCpiUGQLWKFzOkiCAEyPwfEYsL47ySDRg5rMJTaas4F7JA5GiAIFo7i5HJfLSGvcXDL7HRH3C_B_dj9s0-d0gq3NH6aYX5UKpti5kHMvjrQDvM29UqZ_cWEYTWbcYeGO16GDznU_lSgiuFSeoRVU952FQOKT0LLfKaeed_x0goOyEJaMnqg8krYoWFQKlLE8_O5sMhCg4RFYYJny53qo0owL2K11A5PkyoBu8tpGr9GS8xYx4eQLQmQ8_SqM6iw-EiPPw7IlUjkYYtlaYILOYaSBtaWUOKsU7gMao1hQxrwSTqU9AF9RqdyqGoRnAcDpKHAv8F-38v5rAp0JXHe0KrA-jnzXIkB1y0b-mRA_puc7LWyuZc8HWa9pkYFLHkHMS9Ql19cJ55DliKQvXvPDWjOdSeq1CsmtzlDiSUZdPnrxLsyiao8LNihKpx96ElnUC7kTyyDo-d3Cj1Jr7V_YAlNtyLu1Xs1AWxWYRXF=w1263-h865">  
</div>

 
### Gradient Descent
- 최적화 알고리즘의 대표  
- 독립변수 $\theta_1$과 $\theta_2$을 변형시켜가며 오차 함수 $J(\theta_1, \theta_2)$ 의 최소값을 찾는 알고리즘  
  - 오차 함수 $J(\theta_1, \theta_2)$ 의 최소값은 기울기가 0인 지점  
  - 기울기가 양수라는 것은 $\theta$ 값이 커질 수록 함수 값이 커진다는 것을 의미하고, 반대로 기울기가 음수라면 $\theta$값이 커질수록 함수의 값이 작아진다는 것을 의미  

- 오차함수
  - $Error Function = (ŷ^{(i)} - y^{(i)})^2$
    - $ŷ^{(i)}$ : i 번째 사례의 예측 값
    - $y^{(i)}$ : i 번째 사례의 라벨
- 비용함수
  - $MSE(θ,b) = {1 \over m}Σ(ŷ^{(i)} - y^{(i)})^2$  
  $   = {1 \over m}Σ(θx^{(i)}  + b - y^{(i)})^2$
    - $ŷ^{(i)}$ : i 번째 사례의 예측 값
    - $y^{(i)}$ : i 번째 사례의 라벨
    - $x^{(i)}$ : i 번째 입력 데이터 벡터
    - $θ$ : 가중치 벡터 (기울기)
    - $b$ : 편차 (y절편)
- 비용함수를 미분
  - $θ 편미분$
    - ${\partial \over \partial \theta}MSE(θ,b) = {1 \over m}Σ[(θx^{(i)}  + b - y^{(i)})^2]'$  
    $   = {2 \over m}Σ(θx^{(i)}  + b - y^{(i)})[(θx^{(i)}  + b - y^{(i)})]'$  
    $   = {2 \over m}Σ(θx^{(i)}  + b - y^{(i)})x^{(i)}$  
  - $b 편미분$
    - ${\partial \over \partial b}MSE(θ,b) = {1 \over m}Σ[(θx^{(i)}  + b - y^{(i)})^2]'$  
    $   = {2 \over m}Σ(θx^{(i)}  + b - y^{(i)})[(θx^{(i)}  + b - y^{(i)})]'$  
    $   = {2 \over m}Σ(θx^{(i)}  + b - y^{(i)})$  

- 경사하강 알고리즘 과정  
  1. 경사하강법은 임의의 $\theta_1, \theta_2$를 랜덤으로 선택 즉, random initialization을 실행
  2. 반복적으로 파라미터 $\theta_1, \theta_2$를 업데이트 해가며, 오차 함수 $J(\theta_1,\theta_2)$ 값이 낮아지는 방향으로 진행  
  3. 기울기가 커진다는 것은 오차함수 값이 커지는 방향이라는 것과 같기 때문에 경사하강법 알고리즘은 기울기의 반대 방향(기울기 $∇ J(\theta_1)$, $∇ J(\theta_2)$가 작아지는 방향)으로 이동  
  4. 그리고 기울기가 0이 되어 **`global minimum`**에 도달할 때까지 이동  
- 경사하강 알고리즘 수학적 공식
$$\theta_{n+1} = \theta_n - \eta ∇ J(\theta_n)$$
  - $\eta$는 학습률, $∇ J(\theta_n)$는 기울기를 의미

- 편미분 - 전미분의 차이
  - 편미분 : 하나의 변수를 고려한 하나의 접선의 기울기  
  - 전미분 : 다양한 변수에 대해 다양한 접선의 기울기  
    - 각각의 변수에 대한 편미분 뒤 모두 더하기 = 증분


